#!/usr/bin/env python3
"""Visualize performance metrics in a readable format."""

import json
from pathlib import Path
from datetime import datetime

def load_metrics():
    """Load performance metrics from analysis file."""
    metrics_file = Path('/Users/masa/Projects/claude-multiagent-pm/logs/performance_benchmarks_analysis.json')
    with open(metrics_file, 'r') as f:
        return json.load(f)

def print_section(title, width=60):
    """Print a formatted section header."""
    print(f"\n{'=' * width}")
    print(f"{title.upper():^{width}}")
    print(f"{'=' * width}")

def print_subsection(title, width=60):
    """Print a formatted subsection header."""
    print(f"\n{title}")
    print(f"{'-' * len(title)}")

def format_ms(value):
    """Format milliseconds value."""
    if value < 1:
        return f"{value * 1000:.1f}Î¼s"
    elif value < 1000:
        return f"{value:.2f}ms"
    else:
        return f"{value / 1000:.2f}s"

def main():
    """Display performance metrics."""
    metrics = load_metrics()
    perf = metrics['performance_metrics']
    
    print_section("Claude PM Framework Performance Analysis", 70)
    print(f"\nAnalysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. Orchestration Mode Comparison
    print_section("Orchestration Mode Performance", 70)
    
    subprocess = perf['orchestration_modes']['subprocess_mode']['observed_metrics']
    local = perf['orchestration_modes']['local_mode']['observed_metrics']
    
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Metric              â”‚ Subprocess     â”‚ LOCAL Mode     â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Average Time        â”‚ {format_ms(subprocess['average_execution_time_ms']):>14} â”‚ {format_ms(local['average_execution_time_ms']):>14} â”‚")
    print(f"â”‚ Min Time            â”‚ {format_ms(subprocess['min_execution_time_ms']):>14} â”‚ {format_ms(local['min_execution_time_ms']):>14} â”‚")
    print(f"â”‚ Max Time            â”‚ {format_ms(subprocess['max_execution_time_ms']):>14} â”‚ {format_ms(local['max_execution_time_ms']):>14} â”‚")
    print(f"â”‚ Memory Overhead     â”‚ {subprocess['memory_overhead_mb']:>12} MB â”‚ {local['memory_overhead_mb']:>12} MB â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # 2. Performance Improvements
    improvements = perf['performance_improvements']['local_vs_subprocess']
    print_subsection("\nPerformance Improvements")
    print(f"  â€¢ Speedup Factor: {improvements['average_speedup']:.1f}x faster")
    print(f"  â€¢ Improvement: {improvements['percentage_improvement']:.1f}%")
    print(f"  â€¢ Response Time Reduction: {format_ms(improvements['response_time_reduction_ms'])}")
    
    # 3. Component Timing Breakdown
    print_section("Component Timing Analysis", 70)
    
    components = perf['component_timings']
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Component               â”‚ Average Time â”‚ Notes                   â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    
    for comp_name, comp_data in components.items():
        name = comp_data['description'].split()[0:3]
        name = ' '.join(name)[:23]
        time = format_ms(comp_data['average_ms'])
        
        if 'token_reduction_percent' in comp_data:
            note = f"{comp_data['token_reduction_percent']:.1f}% token reduction"
        elif 'impact' in comp_data:
            note = comp_data['impact']
        else:
            note = f"{comp_data.get('samples', 'N/A')} samples"
            
        print(f"â”‚ {name:<23} â”‚ {time:>12} â”‚ {note:<23} â”‚")
    
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # 4. Performance Benchmarks
    print_section("Performance Benchmarks", 70)
    
    benchmarks = perf['benchmarks']
    for bench_name, bench_data in benchmarks.items():
        print_subsection(f"\n{bench_name.replace('_', ' ').title()}")
        print(f"  â€¢ Subprocess: {format_ms(bench_data['subprocess_ms'])}")
        print(f"  â€¢ LOCAL Mode: {format_ms(bench_data['local_ms'])}")
        print(f"  â€¢ Improvement: {bench_data['improvement_factor']:.1f}x faster")
        if 'note' in bench_data:
            print(f"  â€¢ Note: {bench_data['note']}")
    
    # 5. Cache Performance
    print_section("Cache Performance (Claimed)", 70)
    
    cache = perf['cache_performance']['shared_prompt_cache']
    print(f"\n  â€¢ Claim: {cache['claim']}")
    print(f"  â€¢ Status: {cache['validation_status']}")
    print("\n  Expected Metrics:")
    for metric, value in cache['expected_metrics'].items():
        if metric.endswith('_ms'):
            print(f"    - {metric.replace('_', ' ').title()}: {format_ms(value)}")
        else:
            print(f"    - {metric.replace('_', ' ').title()}: {value}%")
    
    # 6. Memory Scaling
    print_section("Memory Usage Patterns", 70)
    
    memory = perf['memory_usage']
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Mode            â”‚ Base Memory    â”‚ Per Agent      â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print(f"â”‚ Subprocess      â”‚ {memory['subprocess_mode']['base_memory_mb']:>12} MB â”‚ {memory['subprocess_mode']['per_agent_mb']:>12} MB â”‚")
    print(f"â”‚ LOCAL           â”‚ {memory['local_mode']['base_memory_mb']:>12} MB â”‚ {memory['local_mode']['per_agent_mb']:>12} MB â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    # 7. Summary and Recommendations
    print_section("Summary & Recommendations", 70)
    
    print("\nğŸ“Š Key Findings:")
    print(f"  âœ“ LOCAL mode is {improvements['average_speedup']:.1f}x faster than subprocess mode")
    print(f"  âœ“ Response time improvement of {improvements['percentage_improvement']:.1f}%")
    print(f"  âœ“ Context filtering reduces tokens by ~62%")
    print(f"  âœ“ Memory usage 25x more efficient in LOCAL mode")
    
    print("\nğŸ’¡ Recommendations:")
    for rec in metrics['recommendations']:
        print(f"  â€¢ {rec}")
    
    # 8. Visual Performance Bar
    print_section("Visual Performance Comparison", 70)
    
    # Create a simple bar chart
    subprocess_bar_length = 50
    local_bar_length = int(50 * (local['average_execution_time_ms'] / subprocess['average_execution_time_ms']))
    
    print(f"\nSubprocess Mode ({format_ms(subprocess['average_execution_time_ms'])})")
    print(f"{'â–ˆ' * subprocess_bar_length}")
    
    print(f"\nLOCAL Mode ({format_ms(local['average_execution_time_ms'])})")
    print(f"{'â–ˆ' * local_bar_length}")
    
    print(f"\nğŸš€ Performance Gain: {subprocess_bar_length - local_bar_length} units ({improvements['percentage_improvement']:.1f}%)")

if __name__ == '__main__':
    main()