# Prompt Optimization User Guide

## Quick Start

The Claude PM Framework automatically optimizes prompts to reduce token usage and improve performance. This guide will help you get the most out of these optimization features.

### Enable Optimization (30 seconds)

```bash
# Enable prompt optimization for all agents
export ENABLE_DYNAMIC_MODEL_SELECTION=true

# That's it! The framework will now automatically optimize prompts
```

## Understanding Prompt Optimization

### What Gets Optimized?

The framework optimizes three key areas:

1. **Task Complexity Analysis** - Understands what you're asking
2. **Model Selection** - Chooses the right AI model for the job
3. **Prompt Templates** - Adjusts instructions based on task needs

### Why Does This Matter?

- **Save Money**: 50-66% reduction in API costs
- **Faster Responses**: Smaller prompts = quicker processing
- **Better Performance**: Right model for the right task
- **No Quality Loss**: Maintains or improves task execution

## Common Use Cases

### 1. Simple Tasks (Use Haiku)

These tasks automatically use the fastest, most efficient model:

```bash
# Examples that will use Haiku:
"List all Python files"
"Check if tests are passing"
"Show current git status"
"Read the README file"
```

**Token Savings**: ~52% reduction

### 2. Standard Tasks (Use Sonnet)

Typical development tasks use the balanced Sonnet model:

```bash
# Examples that will use Sonnet:
"Implement user authentication"
"Fix the bug in payment processing"
"Add unit tests for the API"
"Update documentation for new features"
```

**Token Savings**: ~64% reduction

### 3. Complex Tasks (Use Opus)

Architecture and complex analysis use the most capable model:

```bash
# Examples that will use Opus:
"Refactor the entire authentication system"
"Design a microservices architecture"
"Optimize database performance across all queries"
"Migrate from MongoDB to PostgreSQL"
```

**Token Savings**: ~72% reduction (through smarter prompting)

## Step-by-Step Setup

### Step 1: Check Current Token Usage

Before enabling optimization, capture your baseline:

```bash
# Note your current daily token usage from your API dashboard
# Example: 500,000 tokens/day average
```

### Step 2: Enable for One Agent First

Start with a low-risk agent:

```bash
# Enable just for documentation agent
export CLAUDE_PM_DOCUMENTATION_MODEL_SELECTION=true

# Run for 24 hours and monitor
```

### Step 3: Review Results

After 24 hours, check the improvement:

```python
# In your Python environment
from claude_pm.agents.agent_loader import get_model_selection_metrics

metrics = get_model_selection_metrics()
print(f"Total optimized tasks: {metrics['selection_stats']['total_selections']}")
print(f"Average token reduction: {metrics['selection_stats']['average_token_reduction']}%")
```

### Step 4: Expand Coverage

If results are good, enable for more agents:

```bash
# Add more agents
export CLAUDE_PM_QA_MODEL_SELECTION=true
export CLAUDE_PM_RESEARCH_MODEL_SELECTION=true
export CLAUDE_PM_OPS_MODEL_SELECTION=true
```

### Step 5: Full Rollout

Once confident, enable globally:

```bash
# Enable for all agents
export ENABLE_DYNAMIC_MODEL_SELECTION=true
```

## Configuration Options

### Basic Configuration

Most users only need these settings:

```bash
# Enable optimization (default: false)
export ENABLE_DYNAMIC_MODEL_SELECTION=true

# That's it for most users!
```

### Advanced Configuration

For fine-tuning (optional):

```bash
# Force specific model for an agent
export CLAUDE_PM_ENGINEER_MODEL=claude-4-opus

# Adjust complexity thresholds
export CLAUDE_PM_COMPLEXITY_SIMPLE_THRESHOLD=25  # Default: 30
export CLAUDE_PM_COMPLEXITY_MEDIUM_THRESHOLD=65  # Default: 70

# Disable for specific agents
export CLAUDE_PM_SECURITY_MODEL_SELECTION=false
```

## Monitoring Performance

### Quick Health Check

```python
from claude_pm.agents.agent_loader import get_model_selection_metrics

# Get current optimization status
metrics = get_model_selection_metrics()

# Check if optimization is working
if metrics['feature_flag']['global_enabled']:
    print("‚úÖ Optimization is ENABLED")
    print(f"üìä Tasks optimized: {metrics['selection_stats']['total_selections']}")
    print(f"üí∞ Average savings: {metrics['selection_stats']['average_token_reduction']}%")
else:
    print("‚ùå Optimization is DISABLED")
```

### Detailed Analytics

```python
# See breakdown by model
by_model = metrics['selection_stats']['by_model']
print("\nModel Usage:")
for model, count in by_model.items():
    print(f"  {model}: {count} tasks")

# See breakdown by complexity
complexity = metrics['selection_stats']['complexity_distribution']
print("\nComplexity Distribution:")
print(f"  Simple (0-30): {complexity['0-30']} tasks")
print(f"  Medium (31-70): {complexity['31-70']} tasks")
print(f"  Complex (71-100): {complexity['71-100']} tasks")
```

## Troubleshooting

### Optimization Not Working?

1. **Check Environment Variable**
   ```bash
   echo $ENABLE_DYNAMIC_MODEL_SELECTION
   # Should output: true
   ```

2. **Verify Task Has Description**
   ```bash
   # ‚ùå Won't optimize (no description)
   claude-pm "fix bugs"
   
   # ‚úÖ Will optimize (clear description)
   claude-pm "Fix authentication bug where users can't log in with Google OAuth"
   ```

3. **Check Agent-Specific Settings**
   ```bash
   # See all PM environment variables
   env | grep CLAUDE_PM
   ```

### Wrong Model Being Selected?

Add more context to help the analyzer:

```bash
# Instead of:
"Update the code"

# Use:
"Update authentication code to add two-factor authentication support, modifying 5 files and adding new database tables"
```

### Want to Force a Specific Model?

```python
# In your code
from claude_pm.utils.task_tool_helper import TaskToolHelper

helper = TaskToolHelper()
result = await helper.create_agent_subprocess(
    agent_type="qa",
    task_description="Critical production validation",
    model_override="claude-4-opus"  # Force Opus for critical tasks
)
```

## Tips for Maximum Savings

### 1. Be Specific in Task Descriptions

```bash
# ‚ùå Vague (might use wrong model)
"Fix the thing"

# ‚úÖ Specific (accurate complexity assessment)
"Fix typo in README.md documentation"
```

### 2. Include Relevant Context

```bash
# ‚ùå Missing context
"Implement feature"

# ‚úÖ Context helps optimization
"Implement user profile feature: add 3 new API endpoints, update React components, and modify user database schema"
```

### 3. Use Batch Operations

```bash
# ‚ùå Individual simple tasks (each incurs overhead)
"Check file1.py"
"Check file2.py"
"Check file3.py"

# ‚úÖ Batch simple tasks (one optimized prompt)
"Check all Python files in the src directory for syntax errors"
```

### 4. Let Simple Tasks Be Simple

```bash
# ‚ùå Over-explaining simple tasks
"Please carefully read the README.md file and provide a comprehensive analysis of its contents including structure, clarity, completeness, and suggestions for improvement"

# ‚úÖ Keep simple tasks simple
"Summarize the README.md file"
```

## Cost Savings Calculator

Estimate your savings:

```python
# Your current usage
daily_tokens = 500000  # Your daily average
cost_per_million = 15  # Your current rate

# With optimization (60% average reduction)
optimized_tokens = daily_tokens * 0.4
daily_savings = (daily_tokens - optimized_tokens) * (cost_per_million / 1000000)

print(f"Current daily cost: ${daily_tokens * cost_per_million / 1000000:.2f}")
print(f"Optimized daily cost: ${optimized_tokens * cost_per_million / 1000000:.2f}")
print(f"Daily savings: ${daily_savings:.2f}")
print(f"Monthly savings: ${daily_savings * 30:.2f}")
print(f"Annual savings: ${daily_savings * 365:.2f}")
```

## Frequently Asked Questions

### Q: Will this affect the quality of outputs?

**A**: No. The optimization system maintains or improves quality by matching the right model to each task. Simple tasks don't need complex models, and complex tasks still get the full power they need.

### Q: Can I disable it for specific tasks?

**A**: Yes. You can disable globally, per-agent, or per-task using environment variables or code overrides.

### Q: How much will I actually save?

**A**: Most users see 50-66% reduction in token usage. Actual savings depend on your task distribution. Simple tasks save more, complex tasks save through smarter prompting.

### Q: Is there any performance overhead?

**A**: The analysis adds <1ms per task. With caching, repeated tasks have zero overhead. Most users see faster overall performance due to smaller prompts.

### Q: Do I need to change my code?

**A**: No. The optimization works transparently with existing code. Just set the environment variable and it starts working.

### Q: What if I need a specific model for compliance?

**A**: You can force specific models using `model_override` in code or environment variables per agent.

## Next Steps

1. **Enable optimization** for one agent
2. **Monitor for 24 hours** to see impact
3. **Review metrics** and confirm savings
4. **Expand gradually** to more agents
5. **Enable globally** when confident

Need help? Check the [detailed technical documentation](../features/prompt-optimization.md) or file an issue on GitHub.

---

**Guide Version**: 1.0.0  
**Last Updated**: 2025-07-20  
**For Framework Version**: 1.3.0+